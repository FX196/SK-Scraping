{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils.exceptions import IllegalCharacterError\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TapTapScraper:\n",
    "    def __init__(self):\n",
    "        self.wb = Workbook()\n",
    "        self.ws = self.wb.active\n",
    "        self.ws.title = \"TapTap\"\n",
    "        self.next_row = 1\n",
    "        self.row_dict = {1: 1}\n",
    "\n",
    "    def parse_page(self, page, page_num):\n",
    "        if page_num in self.row_dict.keys():\n",
    "            self.next_row = self.row_dict[page_num]\n",
    "        while page.status_code != 200:\n",
    "            time.sleep(1)\n",
    "            page = get_page(page_num)\n",
    "        page_soup = bs(page.text, 'html.parser')\n",
    "        commentList = page_soup.find_all(class_=\"posts-item-text topic-posts-item-text\")\n",
    "        for comment in commentList:\n",
    "            parsed = self.parse_comment(comment)\n",
    "            if len(parsed[3]) < 40:\n",
    "                continue\n",
    "            text_data, img_data = list(parsed[:-1]), parsed[-1]\n",
    "            for i in range(4):\n",
    "                try:\n",
    "                    self.ws.cell(row=self.next_row, column=(i + 1), value=text_data[i])\n",
    "                except IllegalCharacterError:\n",
    "                    pass\n",
    "            col = 6\n",
    "            self.ws.cell(row=self.next_row, column=5, value=len(img_data))\n",
    "            while img_data:\n",
    "                # save image\n",
    "                img_url = img_data.pop(0)\n",
    "                ext = \"gif\" if img_url.find('.gif') != -1 else \"jpg\"\n",
    "                urllib.request.urlretrieve(img_url,\n",
    "                                           \"/Users/yuhongc/PycharmProjects/SK_Weapon_Event/TapTap_Pictures/%s-%s.%s\" % (\n",
    "                                           parsed[2], col - 5, ext))\n",
    "                # insert to worksheet\n",
    "                self.ws.cell(row=self.next_row, column=col, value=img_url)\n",
    "                col += 1\n",
    "            self.next_row += 1\n",
    "        self.row_dict[page_num + 1] = self.next_row\n",
    "        time.sleep(1)\n",
    "\n",
    "    def parse_comment(self, comment):\n",
    "        comment_soup = bs(str(comment), 'html.parser')\n",
    "\n",
    "        \"\"\"\n",
    "        userList = comment_soup.find_all(class_=\"taptap-user\")  # list of users associated with this comment\n",
    "        str(userList[0]).split('\"')[3]  # first user is original commenter \n",
    "        \"\"\"\n",
    "        tap_user_id = str(comment_soup.find_all(class_='taptap-user')[0]).split('\"')[3]\n",
    "        tap_user_name = comment_soup.find_all(class_=\"taptap-user-name taptap-link\")[0].get_text()\n",
    "        floor_num = int(comment_soup.find_all(class_=\"pull-right\")[0].get_text()[:-1])\n",
    "        comment_body = comment_soup.find_all(class_=\"item-text-body bbcode-body js-open-bbcode-image\")[0]\n",
    "        comment_text = comment_body.prettify()\n",
    "        comment_text = comment_text.replace(\"<br/>\", \"\")\n",
    "        comment_text = remove_tags(comment_text)\n",
    "        comment_img_srcs = [str(img)[str(img).find(\"src\"):].split('\"')[1]\n",
    "                            for img in comment_body.find_all(\"img\")]  # convert to str then locate src\n",
    "\n",
    "        return tap_user_id, tap_user_name, floor_num, comment_text, comment_img_srcs\n",
    "\n",
    "    def save(self, checkpoint=None):\n",
    "        if not checkpoint:\n",
    "            self.wb.save('TapTap.xlsx')\n",
    "        else:\n",
    "            self.wb.save('TapTap-checkpoint-%d.xlsx' % checkpoint)\n",
    "\n",
    "def get_page(page_num):\n",
    "    url = \"https://www.taptap.com/topic/3341168?page=%d#postsList\" % page_num\n",
    "    page = requests.get(url)\n",
    "    return page\n",
    "\n",
    "def remove_tags(text):\n",
    "    while \"<\" in text:\n",
    "        start = text.find(\"<\")\n",
    "        end = text.find(\">\")\n",
    "        text = text[:start] + text[end + 1:]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = get_page(146)\n",
    "page_soup = bs(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://img2.tapimg.com/bbcode/images/8c60997d158241108fb3756399a8ea27.png?imageView2/2/w/1320/q/80/format/jpg/interlace/1/ignore-error/1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentList = page_soup.find_all(class_=\"posts-item-text topic-posts-item-text\")\n",
    "# print(commentList[0])\n",
    "comment = commentList[0]\n",
    "comment_soup = bs(str(comment), 'html.parser')\n",
    "#print(comment_soup.prettify())\n",
    "parsed = Scraper.parse_comment(comment)\n",
    "parsed[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraper = TapTapScraper()\n",
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "Scraped page 6\n",
      "Scraped page 7\n",
      "Scraped page 8\n",
      "Scraped page 9\n",
      "Scraped page 10\n",
      "Scraped page 11\n",
      "Scraped page 12\n",
      "Scraped page 13\n",
      "Scraped page 14\n",
      "Scraped page 15\n",
      "Scraped page 16\n",
      "Scraped page 17\n",
      "Scraped page 18\n",
      "Scraped page 19\n",
      "Scraped page 20\n",
      "Scraped page 21\n",
      "Scraped page 22\n",
      "Scraped page 23\n",
      "Scraped page 24\n",
      "Scraped page 25\n",
      "Scraped page 26\n",
      "Scraped page 27\n",
      "Scraped page 28\n",
      "Scraped page 29\n",
      "Scraped page 30\n",
      "Scraped page 31\n",
      "Scraped page 32\n",
      "Scraped page 33\n",
      "Scraped page 34\n",
      "Scraped page 35\n",
      "Scraped page 36\n",
      "Scraped page 37\n",
      "Scraped page 38\n",
      "Scraped page 39\n",
      "Scraped page 40\n",
      "Scraped page 41\n",
      "Scraped page 42\n",
      "Scraped page 43\n",
      "Scraped page 44\n",
      "Scraped page 45\n",
      "Scraped page 46\n",
      "Scraped page 47\n",
      "Scraped page 48\n",
      "Scraped page 49\n",
      "Scraped page 50\n",
      "Scraped page 51\n",
      "Scraped page 52\n",
      "Scraped page 53\n",
      "Scraped page 54\n",
      "Scraped page 55\n",
      "Scraped page 56\n",
      "Scraped page 57\n",
      "Scraped page 58\n",
      "Scraped page 59\n",
      "Scraped page 60\n",
      "Scraped page 61\n",
      "Scraped page 62\n",
      "Scraped page 63\n",
      "Scraped page 64\n",
      "Scraped page 65\n",
      "Scraped page 66\n",
      "Scraped page 67\n",
      "Scraped page 68\n",
      "Scraped page 69\n",
      "Scraped page 70\n",
      "Scraped page 71\n",
      "Scraped page 72\n",
      "Scraped page 73\n",
      "Scraped page 74\n",
      "Scraped page 75\n",
      "Scraped page 76\n",
      "Scraped page 77\n",
      "Scraped page 78\n",
      "Scraped page 79\n",
      "Scraped page 80\n",
      "Scraped page 81\n",
      "HTTPError detected, re-scraping page\n",
      "Scraped page 82\n",
      "Scraped page 83\n",
      "Scraped page 84\n",
      "Scraped page 85\n",
      "Scraped page 86\n",
      "Scraped page 87\n",
      "Scraped page 88\n",
      "Scraped page 89\n",
      "Scraped page 90\n",
      "Scraped page 91\n",
      "Scraped page 92\n",
      "Scraped page 93\n",
      "Scraped page 94\n",
      "Scraped page 95\n",
      "Scraped page 96\n",
      "Scraped page 97\n",
      "Scraped page 98\n",
      "Scraped page 99\n",
      "Scraped page 100\n",
      "Scraped page 101\n",
      "Scraped page 102\n",
      "Scraped page 103\n",
      "Scraped page 104\n",
      "Scraped page 105\n",
      "Scraped page 106\n",
      "Scraped page 107\n",
      "Scraped page 108\n",
      "Scraped page 109\n",
      "Scraped page 110\n",
      "Scraped page 111\n",
      "Scraped page 112\n",
      "Scraped page 113\n",
      "Scraped page 114\n",
      "Scraped page 115\n",
      "Scraped page 116\n",
      "Scraped page 117\n",
      "Scraped page 118\n",
      "Scraped page 119\n",
      "Scraped page 120\n",
      "Scraped page 121\n",
      "Scraped page 122\n",
      "Scraped page 123\n",
      "Scraped page 124\n",
      "Scraped page 125\n",
      "Scraped page 126\n",
      "Scraped page 127\n",
      "Scraped page 128\n",
      "Scraped page 129\n",
      "Scraped page 130\n",
      "Scraped page 131\n",
      "Scraped page 132\n",
      "Scraped page 133\n",
      "Scraped page 134\n",
      "Scraped page 135\n",
      "Scraped page 136\n",
      "Scraped page 137\n",
      "Scraped page 138\n",
      "Scraped page 139\n",
      "Scraped page 140\n",
      "Scraped page 141\n",
      "Scraped page 142\n",
      "Scraped page 143\n",
      "Scraped page 144\n",
      "Scraped page 145\n",
      "Scraped page 146\n",
      "Scraped page 147\n",
      "Scraped page 148\n",
      "Scraped page 149\n",
      "Scraped page 150\n",
      "Scraped page 151\n",
      "Scraped page 152\n",
      "Scraped page 153\n",
      "Scraped page 154\n",
      "HTTPError detected, re-scraping page\n",
      "Scraped page 155\n",
      "Scraped page 156\n",
      "Scraped page 157\n",
      "Scraped page 158\n",
      "Scraped page 159\n"
     ]
    }
   ],
   "source": [
    "while i < 160:\n",
    "    try:\n",
    "        Scraper.parse_page(get_page(i), i)\n",
    "        print(\"Scraped page %d\" % i)\n",
    "        Scraper.save()\n",
    "        i += 1\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(\"HTTPError detected, re-scraping page\")\n",
    "        continue\n",
    "    except ConnectionError as e:\n",
    "        time.sleep(20)\n",
    "        continue\n",
    "    if i%30 == 0:\n",
    "        Scraper.save(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://img2.tapimg.com/bbcode/images/7fd41fd8afea4dc7939ce5a5a1673b66.gif\", \"local-filename.jpg\")\n",
    "img = Image('local-filename.jpg')\n",
    "# Scraper.ws.add_image(img, 'D5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
